services:
  ollama:
    image: ollama/ollama:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_KEEP_ALIVE=5m  # shorter keep-alive; model evicted from RAM sooner when idle
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    # Health == API up (model presence handled by ollama-init); simple CLI probe
    healthcheck:
      test: ["CMD-SHELL", "/usr/bin/ollama list >/dev/null 2>&1"]
      interval: 10s
      timeout: 3s
      retries: 60
      start_period: 10s

  # Pull PRIMARY_MODEL once; skip if already present. Retries with backoff.
  ollama-init:
    image: curlimages/curl:8.7.1
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      # Provide concrete defaults (avoid host compose interpolation warnings)
      PRIMARY_MODEL: ${PRIMARY_MODEL:-gpt-oss:20b}
      OLLAMA_URL: ${OLLAMA_URL:-http://ollama:11434}
    entrypoint: ["/bin/sh","-lc"]
    command: >
      set -e;
      primary="$$PRIMARY_MODEL"; [ -z "$$PRIMARY_MODEL" ] && primary="gpt-oss:20b";
      url="$$OLLAMA_URL"; [ -z "$$OLLAMA_URL" ] && url="http://ollama:11434";
      echo "[ollama-init] waiting for API at $$url ...";
      for i in $$(seq 1 120); do
        if curl -sf "$$url/api/version" >/dev/null; then echo "[ollama-init] api up"; break; fi;
        sleep 1;
      done;
      echo "[ollama-init] checking model: $$primary";
      if curl -sf -X POST "$$url/api/show" -H "Content-Type: application/json" -d "{\"name\":\"$$primary\"}" >/dev/null; then
        echo "[ollama-init] model already present"; exit 0;
      fi;
      backoff=2; max=5;
      for attempt in $$(seq 1 $$max); do
        echo "[ollama-init] pulling $$primary (attempt $$attempt/$$max)";
        if curl -sf -X POST "$$url/api/pull" -H "Content-Type: application/json" -d "{\"name\":\"$$primary\"}" --no-buffer | sed 's/\\n/\n/g'; then
          echo "[ollama-init] pull complete"; exit 0;
        fi;
        echo "[ollama-init] pull failed, retrying in $${backoff}s";
        sleep $$backoff; backoff=$$((backoff*2)); [ $$backoff -gt 30 ] && backoff=30;
      done;
      echo "[ollama-init] pull failed after retries"; exit 1;
    restart: "no"

  backend:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.backend
      args:
        GIT_SHA: ${GIT_SHA:-local}
    env_file:
      - ../assistant_api/.env.prod
    environment:
      - OPENAI_BASE_URL=http://ollama:11434/v1
      - OPENAI_MODEL=gpt-oss:20b
      - PRIMARY_MODEL=${PRIMARY_MODEL:-gpt-oss:20b}
      - RAG_URL=http://localhost:8000/api/rag/query
      - FALLBACK_API_KEY_FILE=/run/secrets/openai_api_key
      - OPENAI_API_KEY_FILE=/run/secrets/openai_api_key
      - ALLOWED_ORIGINS=https://leok974.github.io,http://localhost:8080,https://assistant.ledger-mind.org
      - BASE_URL_PUBLIC=${BASE_URL_PUBLIC:-http://localhost:8000}
      # (Optional) For bounded model warm wait you can set MODEL_WAIT_MAX_SECONDS=<seconds>
      # - MODEL_WAIT_MAX_SECONDS=180
    depends_on:
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    volumes:
      - ../data:/app/data
    expose:
      - "8000"
    restart: unless-stopped
    # Healthcheck: poll internal ready alias (fast) so nginx can gate startup
    healthcheck:
      # Chat-ready: require a tiny successful generation (no external deps)
      test: ["CMD-SHELL",
        "python - <<'PY'\nimport json,sys,urllib.request\n# Payload for tiny chat\nbody=json.dumps({'messages':[{'role':'user','content':'hi'}]}).encode()\nheaders={'Content-Type':'application/json'}\n# Quick readiness check first (fast fail if server not up)\ntry:\n  urllib.request.urlopen('http://localhost:8000/ready', timeout=5)\nexcept Exception:\n  sys.exit(1)\n# Attempt chat (allow longer for first token)\nreq=urllib.request.Request('http://localhost:8000/chat', data=body, headers=headers, method='POST')\ntry:\n  with urllib.request.urlopen(req, timeout=25) as resp:\n    sys.exit(0 if resp.status==200 else 1)\nexcept Exception:\n  sys.exit(1)\nPY"
      ]
      interval: 30s
      timeout: 30s
      retries: 20
      start_period: 300s
    secrets:
      - openai_api_key

  nginx:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.frontend
      target: frontend-vite-final
      args:
        FRONTEND_DIR: .
        NGINX_CONF: deploy/nginx.conf
        VITE_BUILD_SHA: ${GIT_SHA:-local}
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "8080:80"
      - "8443:443"
    restart: unless-stopped

  warmup:
    image: curlimages/curl:8.7.1
    depends_on:
      backend:
        condition: service_healthy
    entrypoint: ["/bin/sh","-lc"]
    command: >
      set -e;
      echo "[warmup] prime JSON chat";
      curl -m 8 -sf -X POST http://backend:8000/chat \
        -H 'Content-Type: application/json' \
        -d '{"messages":[{"role":"user","content":"hi"}]}' >/dev/null || true;
      echo "[warmup] prime SSE (first 2 lines then close)";
      curl -m 8 -Ns -X POST http://backend:8000/chat/stream \
        -H 'Content-Type: application/json' \
        -d '{"messages":[{"role":"user","content":"hi"}]}' \
        | sed -n '1,2p;q' || true;
      echo "[warmup] done";
    restart: "no"

volumes:
  ollama-data:

secrets:
  openai_api_key:
    file: ../secrets/openai_api_key

networks:
  shared-ollama:
    external: true
