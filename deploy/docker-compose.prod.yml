version: "3.9"

# Production-focused compose (immutable image, no source bind mounts).
services:
  ollama:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 3s
      retries: 5

  ollama-init:
    image: curlimages/curl:8.7.1
    depends_on:
      - ollama
    environment:
      PRIMARY_MODEL: ${PRIMARY_MODEL:-gpt-oss:20b}
    entrypoint: ["/bin/sh", "-lc"]
    command: >-
      set -e;
      echo "[ollama-init] waiting for ollama...";
      until curl -sSf http://ollama:11434/api/tags >/dev/null; do sleep 2; done;
      echo "[ollama-init] pulling ${PRIMARY_MODEL}...";
      curl -sSf -X POST http://ollama:11434/api/pull -H 'Content-Type: application/json' -d "{\"name\":\"${PRIMARY_MODEL}\"}";
      echo "[ollama-init] done."
    restart: "no"

  backend:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.backend
      args:
        GIT_SHA: ${GIT_SHA:-local}
    env_file:
      - ../assistant_api/.env.prod
    environment:
      - OPENAI_BASE_URL=http://ollama:11434/v1
      - OPENAI_MODEL=gpt-oss:20b
      - PRIMARY_MODEL=${PRIMARY_MODEL:-gpt-oss:20b}
      - FALLBACK_API_KEY_FILE=/run/secrets/openai_api_key
      - OPENAI_API_KEY_FILE=/run/secrets/openai_api_key
      - ALLOWED_ORIGINS=https://leok974.github.io,http://localhost:8080,https://assistant.ledger-mind.org
      - BASE_URL_PUBLIC=${BASE_URL_PUBLIC:-http://localhost:8000}
      # (Optional) For bounded model warm wait you can set MODEL_WAIT_MAX_SECONDS=<seconds>
      # - MODEL_WAIT_MAX_SECONDS=180
    depends_on:
      - ollama
      - ollama-init
    volumes:
      - ../data:/app/data
    expose:
      - "8000"
    restart: unless-stopped
    # Healthcheck: poll internal ready alias (fast) so nginx can gate startup
    healthcheck:
      # Use the canonical /ready endpoint (app-level); alias /api/ready is optional
      test: ["CMD-SHELL","python -c \"import urllib.request,sys; sys.exit(0 if urllib.request.urlopen('http://localhost:8000/ready', timeout=3).getcode()==200 else 1)\""]
      interval: 5s
      timeout: 4s
      retries: 60
      # Allow ample time for initial model pull / warm loop before marking unhealthy
      start_period: 200s
    secrets:
      - openai_api_key

  nginx:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.frontend
      target: frontend-vite-final
      args:
        FRONTEND_DIR: .
        NGINX_CONF: deploy/nginx.conf
        VITE_BUILD_SHA: ${GIT_SHA:-local}
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "8080:80"
      - "8443:443"
    restart: unless-stopped

volumes:
  ollama:

secrets:
  openai_api_key:
    file: ../secrets/openai_api_key
