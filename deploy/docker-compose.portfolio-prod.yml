version: "3.9"

# Portfolio Production Deployment Override
# Deploy to: https://www.leoklemet.com (frontend) + https://api.leoklemet.com (backend)
# Frontend: Portfolio build (dist-portfolio/)
# Backend: FastAPI + Ollama

services:
  backend:
    image: ghcr.io/leok974/leo-portfolio/backend:latest
    container_name: portfolio-backend
    environment:
      # Production URLs
      - ALLOWED_ORIGINS=https://www.leoklemet.com,https://api.leoklemet.com
      - BACKEND_URL=https://api.leoklemet.com

      # Database - using persistent volume mount
      - RAG_DB=/data/rag.sqlite
      - RAG_REPOS=leok974/ledger-mind,leok974/leo-portfolio

      # Ollama (primary LLM) - using host infra-ollama on 11434
      - OLLAMA_HOST=host.docker.internal
      - OLLAMA_PORT=11434
      - OPENAI_BASE_URL=http://host.docker.internal:11434/v1
      - OPENAI_MODEL=gpt-oss:20b
      - PRIMARY_MODEL=gpt-oss:20b
      - OPENAI_API_KEY_OLLAMA=ollama
      - EMBED_MODEL_QUERY=openai/text-embedding-3-large

      # OpenAI (fallback) - requires FALLBACK_API_KEY from env or secrets
      - FALLBACK_BASE_URL=https://api.openai.com/v1
      - FALLBACK_MODEL=gpt-4o-mini

      # Cloudflare Access (if enabled)
      - CF_ACCESS_TEAM_DOMAIN=${CF_ACCESS_TEAM_DOMAIN:-}
      - CF_ACCESS_AUD=${CF_ACCESS_AUD:-}
      - ACCESS_ALLOWED_EMAILS=${ACCESS_ALLOWED_EMAILS:-}

      # Figma MCP Integration (Phase 51)
      - FIGMA_PAT=${FIGMA_PAT:-}
      - FIGMA_TEAM_ID=${FIGMA_TEAM_ID:-}
      - FIGMA_TEMPLATE_KEY=${FIGMA_TEMPLATE_KEY:-}

      # Limits
      - MAX_IMAGE_MB=30
      - MAX_VIDEO_MB=200
    volumes:
      - portfolio_rag_data:/data:rw
      - ../data:/app/data:rw
      - ../assistant_api/.env.prod:/app/.env:ro
    ports:
      - "127.0.0.1:8001:8001"
    restart: unless-stopped
    networks:
      default:
      infra_net:
        aliases:
          - portfolio-api.int
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      com.centurylinklabs.watchtower.enable: "true"

  nginx:
    image: ghcr.io/leok974/leo-portfolio/portfolio:latest
    container_name: portfolio-nginx
    pull_policy: always  # Watchtower will update this automatically
    ports:
      - "127.0.0.1:8082:80"
      # Uncomment for HTTPS termination at nginx (if not using Cloudflare Tunnel)
      # - "443:443"
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      default:
      infra_net:
        aliases:
          - portfolio.int
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost/healthz || exit 1"]
      interval: 20s
      timeout: 3s
      retries: 5
    labels:
      com.centurylinklabs.watchtower.enable: "true"
    # Uncomment for TLS certificates
    # volumes (additional):
    #   - ./certs:/etc/nginx/certs:ro

volumes:
  portfolio_rag_data:
    driver: local

networks:
  infra_net:
    external: true
    name: infra_net

# Note: Using host infra-ollama-1 on port 11434 (gpt-oss:20b)
# No need for separate Ollama container

